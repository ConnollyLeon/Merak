{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "213f19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(r'../../')\n",
    "# import Merak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "328d08c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed5309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0721cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertConfig\n",
    "model = BertForMaskedLM(BertConfig(num_hidden_layers=1))\n",
    "print(model)\n",
    "\n",
    "from transformers.utils.fx import symbolic_trace as trace1\n",
    "# from Merak.autoshard.convert import symbolic_trace as trace2\n",
    "from torch.fx import symbolic_trace as trace3 # cannot be used for transformer models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d57fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<built-in method full of type object at 0x1036565d0>, <built-in method full_like of type object at 0x1036565d0>, <built-in method eye of type object at 0x1036565d0>, <built-in method zeros of type object at 0x1036565d0>, <built-in method ones of type object at 0x1036565d0>, <built-in method empty of type object at 0x1036565d0>, <built-in method arange of type object at 0x1036565d0>, <built-in method tensor of type object at 0x1036565d0>}\n",
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connolly/opt/anaconda3/envs/python3/lib/python3.7/site-packages/torch/fx/_symbolic_trace.py:420: UserWarning: Was not able to add assertion to guarantee correct inputs to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  \"Was not able to add assertion to guarantee correct inputs to \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (bert): Module(\n",
       "    (embeddings): Module(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Module(\n",
       "      (layer): Module(\n",
       "        (0): Module(\n",
       "          (attention): Module(\n",
       "            (self): Module(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Module(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): Module(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): Module(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): Module(\n",
       "    (predictions): Module(\n",
       "      (transform): Module(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1(model) # compared to trace2, it cannot capture LinearProxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee010ea7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trace2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/g3z5kxwd1332dnx9742n57hm0000gn/T/ipykernel_90071/2863220499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrace2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trace2' is not defined"
     ]
    }
   ],
   "source": [
    "trace2(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e5db04c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You cannot specify both input_ids and inputs_embeds at the same time",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/g3z5kxwd1332dnx9742n57hm0000gn/T/ipykernel_90346/2138410216.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrace3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/python3/lib/python3.7/site-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \"\"\"\n\u001b[1;32m    855\u001b[0m     \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mGraphModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python3/lib/python3.7/site-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autowrap_search\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0m_autowrap_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autowrap_function_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             self.create_node('output', 'output', (self.create_arg(fn(*args)),), {},\n\u001b[0m\u001b[1;32m    566\u001b[0m                              type_expr=fn.__annotations__.get('return', None))\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python3/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You cannot specify both input_ids and inputs_embeds at the same time"
     ]
    }
   ],
   "source": [
    "trace3(model.bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9385f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProxy(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):\n",
    "        self.module_args = (in_features, out_features, bias, device, dtype)\n",
    "        super(LinearProxy, self).__init__()\n",
    "        global SHOULD_PRINT_LINEAR\n",
    "        if SHOULD_PRINT_LINEAR:\n",
    "            print_rank_0('using linear proxy')\n",
    "            SHOULD_PRINT_LINEAR = False\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self._bias = bias\n",
    "\n",
    "        self.mp_attr = ' '\n",
    "\n",
    "        w = torch.empty(1, 1)\n",
    "#         self.weight = NumelParameter(w)\n",
    "        self.weight.num_element = lambda: self.out_features*self.in_features\n",
    "\n",
    "        if bias:\n",
    "#             self.bias = NumelParameter(torch.zeros(1))\n",
    "            self.bias.num_element = lambda: self.out_features\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "\n",
    "    def build(self, init_args):\n",
    "        if self.mp_attr == ' ':\n",
    "            return torch.nn.Linear(*self.module_args).cuda()\n",
    "        if self.mp_attr.startswith('row'):\n",
    "            init_method = scaled_init_method_normal(*init_args)\n",
    "            if self.mp_attr == 'row_mlp':\n",
    "                return RowPara(self.module_args[0], self.module_args[1], init_method, bias=self._bias)\n",
    "            else:\n",
    "                return RowPara(self.module_args[0], self.module_args[1], init_method, bias=self._bias, need_permute=False)\n",
    "        elif self.mp_attr.startswith('col'):\n",
    "            init_method = init_method_normal(init_args[0])\n",
    "            if self.mp_attr == 'col_mlp':\n",
    "                return ColPara(self.module_args[0], self.module_args[1], init_method, bias=self._bias)\n",
    "            else:\n",
    "                return ColPara(self.module_args[0], self.module_args[1], init_method, bias=self._bias, need_permute=False)\n",
    "        \n",
    "        raise NotImplementedError(f\"Not supported model/tensor parallelism layers {self.mp_attr}\")\n",
    "        \n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # shape = x.shape\n",
    "        # shape[-1] = self.out_features\n",
    "        # if len(x.shape) == 2:\n",
    "        #     return x[:, :1].expand(-1, self.out_features).contiguous()\n",
    "        try:\n",
    "            return x[:, :, :1].expand(-1,-1, self.out_features).contiguous()#, device=x.device)\n",
    "        except IndexError:\n",
    "            return x[:, :1].expand(-1, self.out_features).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0efe5dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "inputs\n",
      "toy_dense1\n",
      "toy_dense2\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "# with 语句替换linear proxy?\n",
    "class Replacer:\n",
    "    def __enter__(self):\n",
    "        print(\"Replacing torch.nn.Linear with Proxy linear.\")\n",
    "        torch.nn.Linear = LinearProxy\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        import importlib\n",
    "        importlib.reload(torch.nn)\n",
    "        print(\"Reloading torch.nn\")\n",
    "        \n",
    "# with Replacer() as replace:\n",
    "#     linear = torch.nn.Linear(10,10)\n",
    "#     print(linear)\n",
    "\n",
    "# print(linear)\n",
    "\n",
    "# linear = torch.nn.Linear(10,10)\n",
    "# print(linear)\n",
    "    \n",
    "print('-'*50)\n",
    "    \n",
    "def proxy_linear(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        with Replacer() as replace:\n",
    "            return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class toyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = torch.nn.Linear(10,10)\n",
    "        self.dense2 = torch.nn.Linear(10,1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out = self.dense1(inputs)\n",
    "        out = self.dense2(out)\n",
    "        return out\n",
    "\n",
    "class toyModule2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.toy = toyModule()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.toy(inputs)\n",
    "\n",
    "class MerakTrainer():\n",
    "    @proxy_linear\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.model = module()\n",
    "    def __repr__(self):\n",
    "        return str(self.model)\n",
    "\n",
    "# model = MerakTrainer(toyModule2)\n",
    "# print(model)\n",
    "\n",
    "model = toyModule2()\n",
    "\n",
    "inputs = torch.rand(1,10)\n",
    "graph = trace3(model).graph\n",
    "for node in graph.nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67a3c751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__main__'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.toy.__module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65189d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "fn: <function BertForMaskedLM.forward at 0x7ff01d3a8cb0>\n",
      "args:[BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    )\n",
      "  )\n",
      "), Proxy(input_ids), Proxy(attention_mask), Proxy(token_type_ids), Proxy(position_ids), Proxy(head_mask), Proxy(inputs_embeds), Proxy(encoder_hidden_states), Proxy(encoder_attention_mask), Proxy(labels), Proxy(output_attentions), Proxy(output_hidden_states), Proxy(return_dict)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You cannot specify both input_ids and inputs_embeds at the same time",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/g3z5kxwd1332dnx9742n57hm0000gn/T/ipykernel_90919/1081947362.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrace3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/python3/lib/python3.7/site-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \"\"\"\n\u001b[1;32m    857\u001b[0m     \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mGraphModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python3/lib/python3.7/site-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0m_autowrap_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autowrap_function_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;31m# print(self.graph)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             self.create_node('output', 'output', (self.create_arg(fn(*args)),), {},\n\u001b[0m\u001b[1;32m    568\u001b[0m                              type_expr=fn.__annotations__.get('return', None))\n\u001b[1;32m    569\u001b[0m             \u001b[0;31m# print(self.graph)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python3/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m         )\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python3/lib/python3.7/site-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mmodule_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m             _autowrap_check(patcher, getattr(getattr(mod, \"forward\", mod), \"__globals__\", {}),\n\u001b[1;32m    555\u001b[0m                             self._autowrap_function_ids)\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_Patcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpatcher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python3/lib/python3.7/site-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mcall_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mmodule_qualified_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_of_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#not self.is_leaf_module(m, module_qualified_name):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'call_module'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_qualified_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python3/lib/python3.7/site-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mmodule_call_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_orig_module_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             _autowrap_check(patcher, getattr(getattr(mod, \"forward\", mod), \"__globals__\", {}),\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python3/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You cannot specify both input_ids and inputs_embeds at the same time"
     ]
    }
   ],
   "source": [
    "trace3(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2763edd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " x\n",
      "False output placeholder output x\n",
      "\n",
      "\n",
      " w1\n",
      "False output placeholder output w1\n",
      "\n",
      "\n",
      " w2\n",
      "False output placeholder output w2\n",
      "\n",
      "\n",
      " neg\n",
      "True output call_function output <built-in method neg of type object at 0x10e90b5d0>\n",
      "False call_method placeholder sum w1\n",
      "\n",
      "\n",
      " cat\n",
      "True output call_function output <built-in method cat of type object at 0x10e90b5d0>\n",
      "False call_method call_function sum <built-in method neg of type object at 0x10e90b5d0>\n",
      "False call_method placeholder sum w2\n",
      "\n",
      "\n",
      " sum_1\n",
      "True output call_method output sum\n",
      "False call_method call_function sum <built-in method cat of type object at 0x10e90b5d0>\n",
      "\n",
      "\n",
      " neg_1\n",
      "True output call_function output <built-in method neg of type object at 0x10e90b5d0>\n",
      "False call_method placeholder sum w1\n",
      "\n",
      "\n",
      " cat_1\n",
      "True output call_function output <built-in method cat of type object at 0x10e90b5d0>\n",
      "False call_method call_function sum <built-in method neg of type object at 0x10e90b5d0>\n",
      "False call_method placeholder sum w2\n",
      "\n",
      "\n",
      " sum_2\n",
      "True output call_method output sum\n",
      "False call_method call_function sum <built-in method cat of type object at 0x10e90b5d0>\n",
      "\n",
      "\n",
      " max_1\n",
      "True output call_function output <built-in method max of type object at 0x10e90b5d0>\n",
      "True call_method call_method sum sum\n",
      "True call_function call_function <built-in method cat of type object at 0x10e90b5d0> <built-in method cat of type object at 0x10e90b5d0>\n",
      "True call_function call_function <built-in method neg of type object at 0x10e90b5d0> <built-in method neg of type object at 0x10e90b5d0>\n",
      "True placeholder placeholder a1 w1\n",
      "True placeholder placeholder a2 w2\n",
      "\n",
      "\n",
      " add\n",
      "True output call_function output <built-in function add>\n",
      "False call_method placeholder sum x\n",
      "False call_method call_function sum <built-in method max of type object at 0x10e90b5d0>\n",
      "\n",
      "\n",
      " max_2\n",
      "True output call_function output <built-in method max of type object at 0x10e90b5d0>\n",
      "True call_method call_method sum sum\n",
      "True call_function call_function <built-in method cat of type object at 0x10e90b5d0> <built-in method cat of type object at 0x10e90b5d0>\n",
      "True call_function call_function <built-in method neg of type object at 0x10e90b5d0> <built-in method neg of type object at 0x10e90b5d0>\n",
      "True placeholder placeholder a1 w1\n",
      "True placeholder placeholder a2 w2\n",
      "\n",
      "\n",
      " add_1\n",
      "True output call_function output <built-in function add>\n",
      "False call_method call_function sum <built-in function add>\n",
      "False call_method call_function sum <built-in method max of type object at 0x10e90b5d0>\n",
      "\n",
      "\n",
      " output\n",
      "True output output output output\n",
      "False call_method call_function sum <built-in function add>\n",
      "[Match(anchor=max_1, nodes_map={output: max_1, sum_1: sum_1, cat: cat, neg: neg, a1: w1, a2: w2}), Match(anchor=max_2, nodes_map={output: max_2, sum_1: sum_2, cat: cat_1, neg: neg_1, a1: w1, a2: w2})]\n",
      "M()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x, w1, w2):\n",
      "    stack = torch.stack([w1, w2])\n",
      "    max_1 = torch.max(stack);  stack = None\n",
      "    add = x + max_1;  x = max_1 = None\n",
      "    stack_1 = torch.stack([w1, w2]);  w1 = w2 = None\n",
      "    max_2 = torch.max(stack_1);  stack_1 = None\n",
      "    add_1 = add + max_2;  add = max_2 = None\n",
      "    return add_1\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.fx import symbolic_trace, replace_pattern\n",
    "\n",
    "\n",
    "'''\n",
    "How to Use the FX Subgraph Rewriter\n",
    "For easy subgraph rewriting, FX exposes the utility function:\n",
    "    replace_pattern(gm : GraphModule,\n",
    "                    pattern : Callable,\n",
    "                    replacement : Callable)\n",
    "                    -> None\n",
    "`replace_pattern` matches all possible non-overlapping sets of operators\n",
    "and their data dependencies (`pattern`) in the Graph of a GraphModule\n",
    "(`gm`), then replaces each of these matched subgraphs with another\n",
    "subgraph (`replacement).\n",
    "The docstring for `replace_pattern` (located in `subgraph_rewriter.py`)\n",
    "gives an in-depth explanation as to how `pattern` and `replacement`\n",
    "should be specified, what happens during pattern matching, and other\n",
    "important technical details. This tutorial, therefore, is only meant to\n",
    "give an overview as to the FX Subgraph Rewriter's basic functionality.\n",
    "Let's go rewrite a Graph!\n",
    "'''\n",
    "\n",
    "# Sample module\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, w1, w2):\n",
    "        val1 = torch.neg(w1)\n",
    "        m1 = torch.cat([val1, w2]).sum()\n",
    "        val2 = torch.neg(w1)\n",
    "        m2 = torch.cat([val2, w2]).sum()\n",
    "        return x + torch.max(m1) + torch.max(m2)\n",
    "\n",
    "# Symbolically trace an instance of `M`\n",
    "traced = symbolic_trace(M())\n",
    "\n",
    "# Define the pattern. The FX Subgraph Rewriter will match all\n",
    "# non-overlapping instances of the pattern in the larger graph.\n",
    "# Note that Pattern-matching is done based on data dependencies,\n",
    "# not Node names. Even though we're operating on Nodes named `a1` and\n",
    "# `a2` instead of `w1` and `w2`, the pattern is still a valid match\n",
    "# for the two instances of `torch.cat([w1, w2]).sum()` above. Only\n",
    "# operations that contribute to the single output value of the pattern\n",
    "# are considered\n",
    "def pattern(a1, a2):\n",
    "    val1 = torch.neg(a1)\n",
    "    return torch.cat([val1, a2]).sum()\n",
    "\n",
    "# Define the replacement (same rules as the pattern)\n",
    "def replacement(w1, w2):\n",
    "    return torch.stack([w1, w2])\n",
    "\n",
    "# Replace `pattern` with `replacement` in `traced`\n",
    "print(replace_pattern(traced, pattern, replacement))\n",
    "print(traced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1624ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx.subgraph_rewriter import _SubgraphMatcher\n",
    "\n",
    "matcher = _SubgraphMatcher(symbolic_trace(pattern).graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4be6fd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\n"
     ]
    }
   ],
   "source": [
    "print(matcher.pattern_anchor.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6bd5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
